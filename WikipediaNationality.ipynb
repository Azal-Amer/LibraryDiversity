{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "api = \"https://www.googleapis.com/books/v1/volumes?q=title:\"\n",
    "isbnAPI = \"https://www.googleapis.com/books/v1/volumes?q=isbn:\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big idea- Using wikipedia pages to infer ethnicity\n",
    "Keywords and permulations\n",
    "- taking the associated catagories that are used, and using nltk's wordnet to find all associated words\n",
    "Searching\n",
    "- opening up the author's wikipedia article, and searching for the frequencies of all the keywords. At the end of the search, whichever catagory has the most mentions, is the catagory the author is sorted into\n",
    "- if there are no mentions, check the author's birthplace and parse that\n",
    "- if no birthplace, people assume whiteness inherently when writting, so the same must be done. That OR the prediction thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amer_\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "asian = np.asarray(['china','japan','korea','taiwan','east-asian','japanese','korean','chinese','taiwanese','india','pakistan','bangledesh','nepal','south-asian','hindi','urdu'])\n",
    "black = np.asarray(['african-american','nigera','ethiopia','egypt','african','nigerian','ethiopian','egyptian'])\n",
    "# black != african, but african is a subset of black.\n",
    "white = np.asarray(['european','caucasian'])\n",
    "hispanic = np.asarray(['hispanic','mexico','spanish','Latino','Latin'])\n",
    "keywords = np.asarray([asian,black,white,hispanic])\n",
    "keywordTitles = ['asian','black','white','hispanic']\n",
    "for j in range(len(keywords)):\n",
    "    for i in range(len(keywords[j])):\n",
    "        for ss in wn.synsets(keywords[j][i]):\n",
    "            keywords[j] = np.append(keywords[j],np.asarray(ss.lemma_names()),axis = None)\n",
    "    keywords[j].flatten()\n",
    "    keywords[j] = list(np.unique(keywords[j]))\n",
    "\n",
    "keywordDF = (pd.DataFrame(keywords).T)\n",
    "\n",
    "keywordDF.columns =keywordTitles\n",
    "# keywordDF.set_index(['asian']).apply(pd.Series.explode).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBookInfo(title):\n",
    "    title = title.replace(' ', '%')\n",
    "    url = api + title\n",
    "    response = urlopen(url)\n",
    "    data = json.loads(response.read())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ryan', 'North']\n",
      "['Rick', 'Riordan']\n",
      "['John', 'Andrew', 'Simpson']\n",
      "['John', 'Green']\n",
      "['Paulo', 'Coelho']\n",
      "['Suzanne', 'Collins']\n",
      "['Harper', 'Lee']\n",
      "['Khaled', 'Hosseini']\n",
      "['Kai', 'Cheng', 'Thom']\n",
      "['Jonny', 'Garza', 'Villa']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>author</th>\n",
       "      <th>last</th>\n",
       "      <th>first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to Invent Everything</td>\n",
       "      <td>Ryan North</td>\n",
       "      <td>North</td>\n",
       "      <td>Ryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lightning Thief, The (Percy Jackson and the Ol...</td>\n",
       "      <td>Rick Riordan</td>\n",
       "      <td>Riordan</td>\n",
       "      <td>Rick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Oxford English Dictionary</td>\n",
       "      <td>John Simpson</td>\n",
       "      <td>Simpson</td>\n",
       "      <td>John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Fault in Our Stars</td>\n",
       "      <td>John Green</td>\n",
       "      <td>Green</td>\n",
       "      <td>John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Alchemist</td>\n",
       "      <td>Paulo Coelho</td>\n",
       "      <td>Coelho</td>\n",
       "      <td>Paulo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Ballad of Songbirds and Snakes (A Hunger G...</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "      <td>Collins</td>\n",
       "      <td>Suzanne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>Lee</td>\n",
       "      <td>Harper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Kite Runner</td>\n",
       "      <td>Khaled Hosseini</td>\n",
       "      <td>Hosseini</td>\n",
       "      <td>Khaled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fierce Femmes and Notorious Liars</td>\n",
       "      <td>Kai Thom</td>\n",
       "      <td>Thom</td>\n",
       "      <td>Kai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fifteen Hundred Miles from the Sun</td>\n",
       "      <td>Jonny Villa</td>\n",
       "      <td>Villa</td>\n",
       "      <td>Jonny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title           author  \\\n",
       "0                           How to Invent Everything       Ryan North   \n",
       "1  Lightning Thief, The (Percy Jackson and the Ol...     Rick Riordan   \n",
       "2                      The Oxford English Dictionary     John Simpson   \n",
       "3                             The Fault in Our Stars       John Green   \n",
       "4                                      The Alchemist     Paulo Coelho   \n",
       "5  The Ballad of Songbirds and Snakes (A Hunger G...  Suzanne Collins   \n",
       "6                              To Kill a Mockingbird       Harper Lee   \n",
       "7                                    The Kite Runner  Khaled Hosseini   \n",
       "8                  Fierce Femmes and Notorious Liars         Kai Thom   \n",
       "9                 Fifteen Hundred Miles from the Sun      Jonny Villa   \n",
       "\n",
       "       last    first  \n",
       "0     North     Ryan  \n",
       "1   Riordan     Rick  \n",
       "2   Simpson     John  \n",
       "3     Green     John  \n",
       "4    Coelho    Paulo  \n",
       "5   Collins  Suzanne  \n",
       "6       Lee   Harper  \n",
       "7  Hosseini   Khaled  \n",
       "8      Thom      Kai  \n",
       "9     Villa    Jonny  "
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = ['How to Invent Everything','Percy Jackson','Oxford English Dictionary','The Fault in Our Stars','The Alchemist','The Hunger Games','To Kill a Mockingbird','The Kite Runner','0994047134','Fifteen Hundred Miles from the Sun']\n",
    "namesDF = pd.DataFrame()\n",
    "for bookTitle in title:\n",
    "    bookInfo = getBookInfo(bookTitle)['items'][0]['volumeInfo']\n",
    "    author = bookInfo['authors'][0].split(' ')\n",
    "    lastName = author[-1]\n",
    "    firstName = author[0]\n",
    "    print(author)\n",
    "    name = firstName + ' ' + lastName\n",
    "    # Taking the first result for the search query which is index 0, getting information about the book and then printing the author of the book.\n",
    "    namesDF = namesDF.append({'Title':bookInfo['title'],'author' : name, 'last':lastName,'first': firstName},ignore_index=True)\n",
    "namesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ryan North\n",
      "Rick Riordan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amer_\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\amer_\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No author found\n",
      "<built-in method title of str object at 0x000001D53E7E9DF0>\n",
      "John Green (author)\n",
      "Paulo Coelho\n",
      "Suzanne Collins\n",
      "Harper Lee\n",
      "Khaled Hosseini\n",
      "Kai Cheng Thom\n",
      "Dean Smith (footballer, born 1971)\n"
     ]
    }
   ],
   "source": [
    "for i,author in enumerate(namesDF['author']):\n",
    "    bookTitle= namesDF['Title'][i]\n",
    "    # author = \n",
    "    try:\n",
    "        page_object = wikipedia.page(wikipedia.search(author)[0])\n",
    "    except Exception as inst:\n",
    "        try:\n",
    "            page_object = wikipedia.page(author + ' (author)')\n",
    "        except Exception as inst:\n",
    "            searches = wikipedia.search(author)[1:]\n",
    "            for search in (searches):\n",
    "                try:\n",
    "                    page_object = wikipedia.page(search)\n",
    "                    if(page_object.content.find(bookTitle) != -1):\n",
    "                        # checks if the page is about the author\n",
    "                        break\n",
    "                except Exception as inst:\n",
    "                    print('No author found')\n",
    "                    page_object = 'nan'\n",
    "                    continue\n",
    "    print(page_object.title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jonny Evans',\n",
       " 'Jonny Gould',\n",
       " 'Jonny Howson',\n",
       " 'Love Island (2015 TV series, series 3)',\n",
       " 'Jonny Bogris',\n",
       " 'Viva Villa!',\n",
       " 'Jonny Z',\n",
       " 'Kaine Kesler Hayden',\n",
       " 'Jonny Hurst',\n",
       " 'Dean Smith (footballer, born 1971)']"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.search('Jonny Villa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rick Riordan']"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = '9781417732470'\n",
    "url = isbnAPI + title\n",
    "response = urlopen(url)\n",
    "data = json.loads(response.read())\n",
    "data['items'][0]['volumeInfo']['authors']\n",
    "# if the isbn lookup doesnt work, then try the title lookup\n",
    "# Some authors don't have a wikipedia page, so we need to search for them"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03b31e0dbf5ee774e49906e1a4e99cf39d3bc46df0e15af7b9d16b1e9b24f047"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
